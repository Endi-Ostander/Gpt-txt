#project_root\config.py –ø—É—Å—Ç

#project_root\README.md –ø—É—Å—Ç

#project_root\requirements.txt
pytest

#project_root\run
from core.learner.trainer import Trainer

def main():
   trainer = Trainer()
    print("–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ò–ò (exit ‚Äî –≤—ã—Ö–æ–¥):")
    while True:
        user_input = input("> ")
        if user_input.lower() == "exit":
           break
        trainer.process_text(user_input)

if __name__ == "__main__":
    main()

#project_root\core\common\log.py
import logging
import os

LOG_DIR = "data/logs"
os.makedirs(LOG_DIR, exist_ok=True)

logging.basicConfig(
    filename=os.path.join(LOG_DIR, "ai.log"),
    filemode='a',
    format='%(asctime)s | %(levelname)s | %(message)s',
    level=logging.DEBUG
)

def log_info(message: str):
    logging.info(message)

def log_error(message: str):
    logging.error(message)

def log_debug(message: str):
    logging.debug(message)

#project_root\core\common\types
from dataclasses import dataclass
from enum import Enum
from typing import List, Dict, Any, Optional


class PhraseType(Enum):
    """–¢–∏–ø—ã —Ñ—Ä–∞–∑, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –ò–ò."""
    STATEMENT = "statement"    # –£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
    QUESTION = "question"      # –í–æ–ø—Ä–æ—Å
    COMMAND = "command"        # –ö–æ–º–∞–Ω–¥–∞
    UNKNOWN = "unknown"        # –ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ


class KnowledgeType(Enum):
    """–¢–∏–ø—ã –∑–Ω–∞–Ω–∏–π, —Å–æ—Ö—Ä–∞–Ω—è–µ–º—ã—Ö –≤ –ø–∞–º—è—Ç—å."""
    FACT = "fact"
    CONCEPT = "concept"
    DEFINITION = "definition"
    RULE = "rule"
    EVENT = "event"


@dataclass
class Phrase:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤—Ö–æ–¥–Ω–æ–π —Ñ—Ä–∞–∑—ã."""
    text: str
    tokens: List[str]
    phrase_type: PhraseType
    intent: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None


@dataclass
class Fact:
    """–§–∞–∫—Ç, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –ø–∞–º—è—Ç—å."""
    id: str
    subject: str
    predicate: str
    obj: str
    source: Optional[str]
    timestamp: str


@dataclass
class KnowledgeEntry:
    """–•—Ä–∞–Ω–∏–ª–∏—â–µ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."""
    id: str
    title: str
    content: str
    type: KnowledgeType
    tags: List[str]
    created: str
#project_root\core\common\utils.py

import json
import os
import re
import uuid
from datetime import datetime, timezone

def clean_text(text: str) -> str:
    """–£–¥–∞–ª—è–µ—Ç –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã, —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã –∏ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É."""
    text = re.sub(r'[^\w\s]', '', text)  # –£–¥–∞–ª–∏—Ç—å –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
    return re.sub(r'\s+', ' ', text).strip().lower()

def normalize_whitespace(text: str) -> str:
    """–ü—Ä–∏–≤–æ–¥–∏—Ç –≤—Å–µ –ø—Ä–æ–±–µ–ª—ã –∫ –æ–¥–Ω–æ–º—É."""
    return re.sub(r'\s+', ' ', text).strip()

def generate_id() -> str:
    """–°–æ–∑–¥–∞—ë—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä."""
    return str(uuid.uuid4())

def timestamp() -> str:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â—É—é –¥–∞—Ç—É –∏ –≤—Ä–µ–º—è –≤ ISO —Ñ–æ—Ä–º–∞—Ç–µ."""
    return datetime.now(timezone.utc).isoformat()

def load_json(filepath: str) -> dict:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç JSON-—Ñ–∞–π–ª –≤ —Å–ª–æ–≤–∞—Ä—å."""
    if not os.path.exists(filepath):
        return {}
    with open(filepath, 'r', encoding='utf-8') as f:
        return json.load(f)

def save_json(filepath: str, data: dict) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–ª–æ–≤–∞—Ä—å –≤ JSON-—Ñ–∞–π–ª."""
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=4, ensure_ascii=False)

def pretty_print(obj):
    """–£–¥–æ–±–Ω—ã–π –≤—ã–≤–æ–¥ —Å–ª–æ–≤–∞—Ä—è."""
    print(json.dumps(obj, indent=4, ensure_ascii=False))

#project_root\core\inputs\html_scraper.py
import requests
from bs4 import BeautifulSoup

class HtmlScraper:
    """
    –ú–æ–¥—É–ª—å –¥–ª—è —Å–±–æ—Ä–∞ —Ç–µ–∫—Å—Ç–æ–≤ —Å HTML-—Å—Ç—Ä–∞–Ω–∏—Ü.
    –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ç–µ–≥–∞ <body>.
    """
    def fetch_text(self, url: str) -> str:
        try:
            response = requests.get(url)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, "html.parser")
                body = soup.body
                return body.get_text(separator=' ', strip=True) if body else ""
            else:
                print(f"[HtmlScraper] –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {response.status_code}")
                return ""
        except Exception as e:
            print(f"[HtmlScraper] –ò—Å–∫–ª—é—á–µ–Ω–∏–µ: {e}")
            return ""

#project_root\core\inputs\text_input.py
class TextInput:
    """
    –ú–æ–¥—É–ª—å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –≤–≤–æ–¥–∞ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
    –ü–æ–∫–∞ –ø—Ä–æ—Å—Ç–æ –∑–∞–≥–ª—É—à–∫–∞ –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ –≤–≤–æ–¥–∞.
    """
    def get_input(self) -> str:
        return input("–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –ò–ò: ")

#project_root\core\learner\curiosity.py
class Curiosity:
    """
    –ú–æ–¥—É–ª—å –ª—é–±–æ–∑–Ω–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ ‚Äî –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç, –∫–æ–≥–¥–∞ –ò–ò –Ω–µ –ø–æ–Ω–∏–º–∞–µ—Ç,
    –∏ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è.
    –ü–æ–∫–∞ –±–∞–∑–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è.
    """
    def __init__(self):
        self.unknown_phrases = []

    def add_unknown(self, phrase: str):
        self.unknown_phrases.append(phrase)
        print(f"[Curiosity] –ù–æ–≤–∞—è –Ω–µ–ø–æ–Ω—è—Ç–Ω–∞—è —Ñ—Ä–∞–∑–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∞: {phrase}")

    def get_questions(self):
        # –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è
        return [f"–ß—Ç–æ –∑–Ω–∞—á–∏—Ç: '{p}'?" for p in self.unknown_phrases]

#project_root\core\learner\reinforce.py
class Reinforce:
    """
    –ú–æ–¥—É–ª—å –¥–ª—è –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –ò–ò.
    –ü–æ–∫–∞ –∑–∞–≥–ª—É—à–∫–∞ ‚Äî —Å—é–¥–∞ –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É —É—Å–∏–ª–µ–Ω–∏—è –∏–∑—É—á–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π,
    –Ω–∞–ø—Ä–∏–º–µ—Ä, —É—Å–∏–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –ø—Ä–∞–≤–∏–ª –∏–ª–∏ —Ñ–∞–∫—Ç–æ–≤ –ø–æ —á–∞—Å—Ç–æ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.
    """
    def __init__(self):
        self.rewards = {}

    def reinforce_fact(self, fact_id: str):
        self.rewards[fact_id] = self.rewards.get(fact_id, 0) + 1
        print(f"[Reinforce] –£—Å–∏–ª–µ–Ω–∏–µ —Ñ–∞–∫—Ç–∞ {fact_id}: {self.rewards[fact_id]}")

    def get_reward(self, fact_id: str) -> int:
        return self.rewards.get(fact_id, 0)

#project_root\core\learner\rules.py
from typing import List, Optional, Dict, Any
from core.common.types import PhraseType

class Rule:
    """
    –ü—Ä–∞–≤–∏–ª–æ –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ —Ñ—Ä–∞–∑—ã –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤ –∑–Ω–∞–Ω–∏–µ –∏–ª–∏ –¥–µ–π—Å—Ç–≤–∏–µ.
    """
    def __init__(self, name: str, condition: callable, action: callable):
        self.name = name
        self.condition = condition  # –§—É–Ω–∫—Ü–∏—è: –ø—Ä–∏–Ω–∏–º–∞–µ—Ç (phrase_type, tokens, raw_text), –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç bool
        self.action = action        # –§—É–Ω–∫—Ü–∏—è: –ø—Ä–∏–Ω–∏–º–∞–µ—Ç (phrase_type, tokens, raw_text), –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç


class RulesEngine:
    def __init__(self):
        self.rules: List[Rule] = []

    def add_rule(self, rule: Rule):
        self.rules.append(rule)

    def apply_rules(self, phrase_type: PhraseType, tokens: List[str], raw_text: str) -> Optional[Dict[str, Any]]:
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç –ø—Ä–∞–≤–∏–ª–∞ –∫ —Ñ—Ä–∞–∑–µ. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–µ—Ä–≤–æ–≥–æ –ø—Ä–∏–º–µ–Ω–∏–º–æ–≥–æ –ø—Ä–∞–≤–∏–ª–∞.
        """
        for rule in self.rules:
            if rule.condition(phrase_type, tokens, raw_text):
                return rule.action(phrase_type, tokens, raw_text)
        return None


# –£—Å–ª–æ–≤–∏–µ –∏ –¥–µ–π—Å—Ç–≤–∏–µ –¥–ª—è —Ñ–∞–∫—Ç–∞ —Å 3 –∏ –±–æ–ª–µ–µ —Ç–æ–∫–µ–Ω–∞–º–∏
def condition_fact_long(phrase_type: PhraseType, tokens: List[str], raw_text: str) -> bool:
    return phrase_type == PhraseType.STATEMENT and len(tokens) >= 3

def action_fact_long(phrase_type: PhraseType, tokens: List[str], raw_text: str) -> Dict[str, Any]:
    subject = tokens[0]
    predicate = tokens[1]
    obj = " ".join(tokens[2:])
    return {"type": "fact", "subject": subject, "predicate": predicate, "object": obj, "source": raw_text}

# –£—Å–ª–æ–≤–∏–µ –∏ –¥–µ–π—Å—Ç–≤–∏–µ –¥–ª—è —Ñ–∞–∫—Ç–∞ —Å —Ä–æ–≤–Ω–æ 2 —Ç–æ–∫–µ–Ω–∞–º–∏
def condition_fact_short(phrase_type: PhraseType, tokens: List[str], raw_text: str) -> bool:
    return phrase_type == PhraseType.STATEMENT and len(tokens) == 2

def action_fact_short(phrase_type: PhraseType, tokens: List[str], raw_text: str) -> Dict[str, Any]:
    subject = tokens[0]
    predicate = tokens[1]
    obj = ""  # –û–±—ä–µ–∫—Ç–∞ –Ω–µ—Ç
    return {"type": "fact", "subject": subject, "predicate": predicate, "object": obj, "source": raw_text}


# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–≤–∏–∂–∫–∞ –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª –≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
rules_engine = RulesEngine()
rules_engine.add_rule(Rule("fact_rule_long", condition_fact_long, action_fact_long))
rules_engine.add_rule(Rule("fact_rule_short", condition_fact_short, action_fact_short))

#project_root\core\learner\trainer.py
from core.processor.tokenizer import Tokenizer
from core.processor.classifier import Classifier
from core.memory.memory import Memory
from core.common.types import PhraseType
from core.learner.rules import rules_engine


class Trainer:
    def __init__(self):
        self.tokenizer = Tokenizer()
        self.classifier = Classifier()
        self.memory = Memory()

    def process_text(self, text: str):
        """–ì–ª–∞–≤–Ω–∞—è —Ç–æ—á–∫–∞ –æ–±—É—á–µ–Ω–∏—è: –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        phrase_type = self.classifier.classify(text)
        tokens = self.tokenizer.tokenize(text)

        print(f"[Trainer] –¢–∏–ø —Ñ—Ä–∞–∑—ã: {phrase_type.name}")
        print(f"[Trainer] –¢–æ–∫–µ–Ω—ã: {tokens}")

        result = rules_engine.apply_rules(phrase_type, tokens, text)
        if result:
            if result.get("type") == "fact":
                added = self.memory.add_fact(
                    result["subject"],
                    result["predicate"],
                    result["object"],
                    source=result.get("source")
                )
                if added:
                    print(f"[Trainer] –ó–∞–ø–æ–º–Ω–µ–Ω–æ: {result['subject']} ‚Äî {result['predicate']} ‚Äî {result['object']}")
            else:
                # –ú–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–ª—è –¥—Ä—É–≥–∏—Ö —Ç–∏–ø–æ–≤ –ø—Ä–∞–≤–∏–ª –∑–¥–µ—Å—å
                print(f"[Trainer] –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –ø—Ä–∞–≤–∏–ª–æ —Ç–∏–ø–∞: {result.get('type')}")
        else:
            print("[Trainer] –ü—Ä–∞–≤–∏–ª–∞ –Ω–µ –ø—Ä–∏–º–µ–Ω–∏–º—ã –∫ –¥–∞–Ω–Ω–æ–π —Ñ—Ä–∞–∑–µ.")

#project_root\core\memory\updata.py
from core.memory.memory import Memory
from core.common.types import Fact

class UpdateMemory:
    """
    –ú–æ–¥—É–ª—å –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∞–∫—Ç–æ–≤ –≤ –ø–∞–º—è—Ç–∏.
    –ü–æ–∫–∞ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –ø—Ä–æ—Å—Ç–∞—è –∑–∞–º–µ–Ω–∞ —Ñ–∞–∫—Ç–∞ –ø–æ ID.
    """
    def __init__(self):
        self.memory = Memory()

    def update_fact(self, fact_id: str, new_subject: str, new_predicate: str, new_obj: str):
        updated = False
        for i, fact in enumerate(self.memory.facts):
            if fact.id == fact_id:
                self.memory.facts[i] = Fact(fact_id, new_subject, new_predicate, new_obj, fact.source, fact.timestamp)
                updated = True
                break
        if updated:
            self.memory._save_facts()
            print(f"[UpdateMemory] –§–∞–∫—Ç {fact_id} –æ–±–Ω–æ–≤–ª–µ–Ω.")
        else:
            print(f"[UpdateMemory] –§–∞–∫—Ç {fact_id} –Ω–µ –Ω–∞–π–¥–µ–Ω.")

#project_root\core\memory\recall.py
from core.memory.memory import Memory

class Recall:
    """
    –ú–æ–¥—É–ª—å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –ø–∞–º—è—Ç–∏.
    –î–µ–ª–µ–≥–∏—Ä—É–µ—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç—É Memory.
    """
    def __init__(self):
        self.memory = Memory()

    def recall_facts(self, subject: str):
        return self.memory.find_facts_by_subject(subject)

    def recall_knowledge(self, tag: str):
        return self.memory.find_knowledge_by_tag(tag)

#project_root\core\memory\memory.py
import os
from typing import List, Optional
from core.common.utils import load_json, save_json, generate_id, timestamp
from core.common.types import Fact, KnowledgeEntry, KnowledgeType

MEMORY_FILE = "data/memory.json"
KNOWLEDGE_FILE = "data/knowledge_base.json"


class Memory:
    def __init__(self, memory_path: Optional[str] = None, knowledge_path: Optional[str] = None):
        self.memory_path = memory_path or MEMORY_FILE
        self.knowledge_path = knowledge_path or KNOWLEDGE_FILE

        self.facts = self._load_facts()
        self.knowledge = self._load_knowledge()

    def _load_facts(self) -> List[Fact]:
        if not os.path.exists(self.memory_path):
            # –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
            return []
        
        data = load_json(self.memory_path)
        return [Fact(**f) for f in data.get("facts", [])]

    def _load_knowledge(self) -> List[KnowledgeEntry]:
        if not os.path.exists(self.knowledge_path):
            # –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
            return []
        
        data = load_json(self.knowledge_path)
        return [KnowledgeEntry(**k) for k in data.get("entries", [])]

    def _save_facts(self):
        data = {"facts": [f.__dict__ for f in self.facts]}
        save_json(self.memory_path, data)

    def _save_knowledge(self):
        data = {
            "entries": [
                {
                    **k.__dict__,
                    "type": k.type.value if hasattr(k.type, "value") else k.type
                }
                for k in self.knowledge
            ]
        }
        save_json(self.knowledge_path, data)

    def add_fact(self, subject: str, predicate: str, obj: str, source: Optional[str] = None) -> bool:
        for f in self.facts:
            if f.subject == subject and f.predicate == predicate and f.obj == obj:
                print(f"[Memory] –§–∞–∫—Ç —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {subject} ‚Äî {predicate} ‚Äî {obj}")
                return False

        fact = Fact(
            id=generate_id(),
            subject=subject,
            predicate=predicate,
            obj=obj,
            source=source,
            timestamp=timestamp()
        )
        self.facts.append(fact)
        self._save_facts()
        print(f"[Memory] –ù–æ–≤—ã–π —Ñ–∞–∫—Ç –¥–æ–±–∞–≤–ª–µ–Ω: {subject} ‚Äî {predicate} ‚Äî {obj}")
        return True

    def add_knowledge(self, title: str, content: str, k_type: KnowledgeType, tags: List[str]):
        entry = KnowledgeEntry(
            id=generate_id(),
            title=title,
            content=content,
            type=k_type,
            tags=tags,
            created=timestamp()
        )
        self.knowledge.append(entry)
        self._save_knowledge()

    def find_facts_by_subject(self, subject: str) -> List[Fact]:
        return [f for f in self.facts if f.subject == subject]

    def find_knowledge_by_tag(self, tag: str) -> List[KnowledgeEntry]:
        return [k for k in self.knowledge if tag in k.tags]

#project_root\core\processor\classifier.py
from core.common.types import PhraseType


class Classifier:
    def __init__(self):
        self.question_starters = {"—á—Ç–æ", "–∫—Ç–æ", "–≥–¥–µ", "–ø–æ—á–µ–º—É", "–∑–∞—á–µ–º", "–∫–∞–∫", "–∫–æ–≥–¥–∞", "—Å–∫–æ–ª—å–∫–æ", "–º–æ–∂–Ω–æ", "–ª–∏"}
        self.command_verbs = {"—Ä–∞—Å—Å–∫–∞–∂–∏", "–ø–æ–∫–∞–∂–∏", "—Å–∫–∞–∂–∏", "–æ–±—ä—è—Å–Ω–∏", "–∑–∞–ø–æ–º–Ω–∏", "–æ—Ç–≤–µ—Ç—å", "–Ω–∞–π–¥–∏"}

    def classify(self, text: str) -> PhraseType:
        lowered = text.strip().lower()

        if all(c == '?' for c in lowered):
            return PhraseType.UNKNOWN

        if lowered.endswith("?"):
            return PhraseType.QUESTION

        if any(lowered.startswith(q) for q in self.question_starters):
            return PhraseType.QUESTION

        if any(lowered.startswith(cmd) for cmd in self.command_verbs):
            return PhraseType.COMMAND

        if lowered.endswith(".") or " " in lowered:
            return PhraseType.STATEMENT

        return PhraseType.UNKNOWN

#project_root\core\processor\extractor.py
from typing import Dict, Any, Optional

class Extractor:
    """
    –ú–æ–¥—É–ª—å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ —Ç–æ–∫–µ–Ω–æ–≤ –∏ —Ç–µ–∫—Å—Ç–∞.
    –ù–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ ‚Äî –ø—Ä–æ—Å—Ç–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å.
    """
    def extract_fact(self, tokens: list, raw_text: str) -> Optional[Dict[str, Any]]:
        if len(tokens) < 3:
            return None
        return {
            "subject": tokens[0],
            "predicate": tokens[1],
            "object": " ".join(tokens[2:]),
            "source": raw_text
        }

#project_root\core\processor\parser.py
from typing import List
from core.common.types import Phrase

class Parser:
    """
    –ú–æ–¥—É–ª—å —Ä–∞–∑–±–æ—Ä–∞ —Ñ—Ä–∞–∑—ã –Ω–∞ —Å–æ—Å—Ç–∞–≤–Ω—ã–µ —á–∞—Å—Ç–∏.
    –ü–æ–∫–∞ –ø—Ä–æ—Å—Ç–∞—è –∑–∞–≥–ª—É—à–∫–∞ ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –∫–∞–∫ —Ñ—Ä–∞–∑—É —Å –ø—É—Å—Ç—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏.
    –ü–æ–∑–∂–µ –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–π —Ä–∞–∑–±–æ—Ä.
    """
    def parse(self, text: str) -> Phrase:
        return Phrase(text=text, tokens=text.split(), phrase_type=None)

#project_root\core\processor\tokenizer.py
import re
from typing import List
from core.common.utils import clean_text


class Tokenizer:
    def __init__(self):
        # –ú–æ–∂–Ω–æ –ø–æ–∑–∂–µ –¥–æ–±–∞–≤–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤
        self.delimiters = r"[ \t\n\r\f\v.,!?;:\"()\-‚Äî]+"  # –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ —Å–ª–æ–≤

    def tokenize(self, text: str) -> List[str]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —Å—Ç—Ä–æ–∫—É –Ω–∞ —Ç–æ–∫–µ–Ω—ã."""
        cleaned = clean_text(text)
        tokens = re.split(self.delimiters, cleaned)
        return [t for t in tokens if t]  # –£–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏

    def count_tokens(self, text: str) -> int:
        """–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤."""
        return len(self.tokenize(text))

    def preview_tokens(self, text: str) -> None:
        """–í—ã–≤–æ–¥–∏—Ç —Ç–æ–∫–µ–Ω—ã –≤ –∫–æ–Ω—Å–æ–ª—å –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏."""
        tokens = self.tokenize(text)
        print(f"[Tokenizer] ‚Üí {tokens}")

#project_root\tests\__init.py –ø—É—Å—Ç

#project_root\tests\test_classifier.py
import pytest
from core.processor.classifier import Classifier
from core.common.types import PhraseType

classifier = Classifier()

@pytest.mark.parametrize("text,expected", [
    ("–ß—Ç–æ —Ç—ã –∑–Ω–∞–µ—à—å?", PhraseType.QUESTION),
    ("–°–∫–æ–ª—å–∫–æ –≤—Ä–µ–º–µ–Ω–∏?", PhraseType.QUESTION),
    ("–†–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ —Å–æ–ª–Ω—Ü–µ", PhraseType.COMMAND),
    ("–°–æ–ª–Ω—Ü–µ ‚Äî —ç—Ç–æ –∑–≤–µ–∑–¥–∞.", PhraseType.STATEMENT),
    ("???", PhraseType.UNKNOWN),
])
def test_classify(text, expected):
    assert classifier.classify(text) == expected

#project_root\tests\test_memory.py
import tempfile
import shutil
import os
from core.memory.memory import Memory
from core.common.types import KnowledgeType

def test_memory_fact_storage():
    tmp_dir = tempfile.mkdtemp()
    memory_file = os.path.join(tmp_dir, "memory.json")
    knowledge_file = os.path.join(tmp_dir, "knowledge_base.json")

    # –°–æ–∑–¥–∞—ë–º –ø—É—Å—Ç—ã–µ —Ñ–∞–π–ª—ã –¥–ª—è –ø–∞–º—è—Ç–∏
    with open(memory_file, "w", encoding="utf-8") as f:
        f.write('{"facts": []}')
    with open(knowledge_file, "w", encoding="utf-8") as f:
        f.write('{"entries": []}')

    # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç–∏
    memory = Memory(memory_path=memory_file, knowledge_path=knowledge_file)

    # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∞–∫—Ç
    memory.add_fact("–¢–µ—Å—Ç", "–µ—Å—Ç—å", "–¥–∞–Ω–Ω—ã–µ", "—Ç–µ—Å—Ç")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–∞–∫—Ç –±—ã–ª –¥–æ–±–∞–≤–ª–µ–Ω
    facts = memory.find_facts_by_subject("–¢–µ—Å—Ç")
    assert len(facts) == 1
    assert facts[0].predicate == "–µ—Å—Ç—å"

    shutil.rmtree(tmp_dir)

def test_memory_knowledge_storage():
    tmp_dir = tempfile.mkdtemp()
    memory_file = os.path.join(tmp_dir, "memory.json")
    knowledge_file = os.path.join(tmp_dir, "knowledge_base.json")

    # –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    with open(memory_file, "w", encoding="utf-8") as f:
        f.write('{"facts": []}')
    with open(knowledge_file, "w", encoding="utf-8") as f:
        f.write('{"entries": []}')

    # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç–∏
    memory = Memory(memory_path=memory_file, knowledge_path=knowledge_file)

    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞–Ω–∏—è
    memory.add_knowledge("–¢–µ—Å—Ç", "–û–ø–∏—Å–∞–Ω–∏–µ", KnowledgeType.FACT, ["—Ç–µ—Å—Ç"])
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∑–Ω–∞–Ω–∏—è –¥–æ–±–∞–≤–ª–µ–Ω—ã
    found = memory.find_knowledge_by_tag("—Ç–µ—Å—Ç")
    assert len(found) == 1

    shutil.rmtree(tmp_dir)

#project_root\tests\test_tokenizer.py
import pytest
from core.processor.tokenizer import Tokenizer

tokenizer = Tokenizer()

def test_tokenize_basic():
    text = "–ü—Ä–∏–≤–µ—Ç, –º–∏—Ä!"
    tokens = tokenizer.tokenize(text)
    assert tokens == ["–ø—Ä–∏–≤–µ—Ç", "–º–∏—Ä"]

def test_tokenize_empty():
    assert tokenizer.tokenize("") == []

def test_tokenize_whitespace():
    assert tokenizer.tokenize("   \n\t") == []

#project_root\data\knowledge_base.json —Ä–∞–±–æ—Ç–∞–µ—Ç

#project_root\data\memory.json —Ä–∞–±–æ—Ç–∞–µ—Ç

#project_root\data\logs –ø—É—Å—Ç

#project_root\tools\devconsole_ext.py
import sys
import os
import csv
import subprocess

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ sys.path –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –∏–º–ø–æ—Ä—Ç–æ–≤
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from core.memory.memory import Memory
from core.processor.tokenizer import Tokenizer
from core.processor.classifier import Classifier
from core.common.types import KnowledgeType

def resource_path(relative_path):
    """–ü–æ–ª—É—á–∏—Ç—å –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å –∫ —Ä–µ—Å—É—Ä—Å—É, –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –ø–∞–ø–∫–∏ —Å–∫—Ä–∏–ø—Ç–∞."""
    return os.path.abspath(os.path.join(os.path.dirname(__file__), relative_path))

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
memory = Memory(
    memory_path=resource_path("../data/memory.json"),
    knowledge_path=resource_path("../data/knowledge_base.json")
)

tokenizer = Tokenizer()
classifier = Classifier()

HELP = """
üß™ –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è Dev Console ‚Äî –∫–æ–º–∞–Ω–¥—ã:

  –û–±—â–∏–µ:
    help                 ‚Äî –ø–æ–∫–∞–∑–∞—Ç—å —ç—Ç–æ—Ç —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫
    exit                 ‚Äî –≤—ã–π—Ç–∏ –∏–∑ –∫–æ–Ω—Å–æ–ª–∏
    run                  ‚Äî –∑–∞–ø—É—Å—Ç–∏—Ç—å run.py

  –ü—Ä–æ—Å–º–æ—Ç—Ä –∏ –ø–æ–∏—Å–∫:
    mem facts            ‚Äî –≤—ã–≤–µ—Å—Ç–∏ –≤—Å–µ —Ñ–∞–∫—Ç—ã
    mem facts <—Å–ª–æ–≤–æ>    ‚Äî –≤—ã–≤–µ—Å—Ç–∏ —Ñ–∞–∫—Ç—ã –ø–æ —Å—É–±—ä–µ–∫—Ç—É <—Å–ª–æ–≤–æ>
    mem knowledge        ‚Äî –≤—ã–≤–µ—Å—Ç–∏ –≤—Å–µ –∑–Ω–∞–Ω–∏—è
    mem knowledge <—Ç–µ–≥>  ‚Äî –≤—ã–≤–µ—Å—Ç–∏ –∑–Ω–∞–Ω–∏—è –ø–æ —Ç–µ–≥—É <—Ç–µ–≥>

  –†–∞–±–æ—Ç–∞ —Å –ø–∞–º—è—Ç—å—é:
    add fact <subj> <pred> <obj> [source]   ‚Äî –¥–æ–±–∞–≤–∏—Ç—å —Ñ–∞–∫—Ç
    del fact <id>                           ‚Äî —É–¥–∞–ª–∏—Ç—å —Ñ–∞–∫—Ç –ø–æ id

  –≠–∫—Å–ø–æ—Ä—Ç:
    mem export md        ‚Äî —ç–∫—Å–ø–æ—Ä—Ç –ø–∞–º—è—Ç–∏ –≤ Markdown (memory_report.md)
    mem export csv       ‚Äî —ç–∫—Å–ø–æ—Ä—Ç –ø–∞–º—è—Ç–∏ –≤ CSV (memory_report.csv)

  –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ:
    test <—Ñ—Ä–∞–∑–∞>        ‚Äî —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è

  –ü—Ä–∏–º–µ—Ä:
    add fact —Å–æ–ª–Ω—Ü–µ –µ—Å—Ç—å –∑–≤–µ–∑–¥–∞ –í–∏–∫–∏–ø–µ–¥–∏—è
"""

def print_facts(filter_subj=None):
    filtered = memory.facts
    if filter_subj:
        filtered = [f for f in filtered if filter_subj.lower() in f.subject.lower()]
    if not filtered:
        print("‚ùå –§–∞–∫—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        return
    for f in filtered:
        print(f"- ID:{f.id} | {f.subject} ‚Äî {f.predicate} ‚Äî {f.obj} | from: {f.source or 'n/a'}")

def print_knowledge(filter_tag=None):
    filtered = memory.knowledge
    if filter_tag:
        filtered = [k for k in filtered if filter_tag.lower() in map(str.lower, k.tags)]
    if not filtered:
        print("‚ùå –ó–Ω–∞–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        return
    for k in filtered:
        print(f"- ID:{k.id} | {k.title} [{k.type.value}] ‚Äî {', '.join(k.tags)}")
        print(f"  > {k.content}")

def export_md():
    with open("memory_report.md", "w", encoding="utf-8") as f:
        f.write("# üß† –ü–∞–º—è—Ç—å –ò–ò\n\n## üìå –§–∞–∫—Ç—ã\n\n")
        for fact in memory.facts:
            f.write(f"- **{fact.subject}** ‚Äî *{fact.predicate}* ‚Äî {fact.obj}\n")
            f.write(f"  –ò—Å—Ç–æ—á–Ω–∏–∫: `{fact.source}`\n")
            f.write(f"  –í—Ä–µ–º—è: `{fact.timestamp}`\n\n")
        f.write("## üìö –ó–Ω–∞–Ω–∏—è\n\n")
        for entry in memory.knowledge:
            f.write(f"- **{entry.title}** ({entry.type.value})\n")
            f.write(f"  > {entry.content}\n")
            f.write(f"  –¢–µ–≥–∏: {', '.join(entry.tags)}\n")
            f.write(f"  –í—Ä–µ–º—è: `{entry.created}`\n\n")
    print("‚úÖ –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ memory_report.md")

def export_csv():
    with open("memory_report.csv", "w", encoding="utf-8", newline='') as f:
        writer = csv.writer(f)
        writer.writerow(["Type", "ID", "Subject/Title", "Predicate", "Object/Content", "Source/Tags", "Timestamp/Created"])
        for fact in memory.facts:
            writer.writerow(["Fact", fact.id, fact.subject, fact.predicate, fact.obj, fact.source or "", fact.timestamp])
        for entry in memory.knowledge:
            tags = ";".join(entry.tags)
            writer.writerow(["Knowledge", entry.id, entry.title, "", entry.content, tags, entry.created])
    print("‚úÖ –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ memory_report.csv")

def add_fact(args):
    if len(args) < 3:
        print("‚ö†Ô∏è –ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 3 –∞—Ä–≥—É–º–µ–Ω—Ç–∞: subject predicate object [source]")
        return
    subject = args[0]
    predicate = args[1]
    rest = args[2:]
    obj = " ".join(rest[:-1]) if len(rest) > 1 else rest[0]
    source = rest[-1] if len(rest) > 1 else None
    memory.add_fact(subject, predicate, obj, source)
    print(f"‚úÖ –§–∞–∫—Ç –¥–æ–±–∞–≤–ª–µ–Ω: {subject} ‚Äî {predicate} ‚Äî {obj}")

def del_fact(args):
    if len(args) != 1:
        print("‚ö†Ô∏è –ù—É–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å ID —Ñ–∞–∫—Ç–∞ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è.")
        return
    fid = args[0]
    before = len(memory.facts)
    memory.facts = [f for f in memory.facts if f.id != fid]
    memory._save_facts()
    if len(memory.facts) < before:
        print(f"‚úÖ –§–∞–∫—Ç —Å ID {fid} —É–¥–∞–ª—ë–Ω.")
    else:
        print(f"‚ùå –§–∞–∫—Ç —Å ID {fid} –Ω–µ –Ω–∞–π–¥–µ–Ω.")

def main():
    print(HELP)
    while True:
        try:
            line = input("> ").strip()
            if not line:
                continue

            if line.lower() == "exit":
                break
            elif line.lower() == "help":
                print(HELP)
                continue
            elif line.lower() == "run":
                # –ó–∞–ø—É—Å–∫–∞–µ–º run.py –∏–∑ –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞ (–Ω–∞ —É—Ä–æ–≤–µ–Ω—å –≤—ã—à–µ)
                run_path = resource_path("../run.py")
                if os.path.exists(run_path):
                    subprocess.run([sys.executable, run_path])
                else:
                    print(f"‚ö†Ô∏è –§–∞–π–ª run.py –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏: {run_path}")
                continue

            parts = line.split()
            cmd = parts[0].lower()

            if cmd == "mem":
                if len(parts) < 2:
                    print("‚ö†Ô∏è –£–∫–∞–∂–∏—Ç–µ –∞—Ä–≥—É–º–µ–Ω—Ç: facts, knowledge –∏–ª–∏ export")
                    continue
                subcmd = parts[1].lower()
                if subcmd == "facts":
                    subj = parts[2] if len(parts) > 2 else None
                    print_facts(subj)
                elif subcmd == "knowledge":
                    tag = parts[2] if len(parts) > 2 else None
                    print_knowledge(tag)
                elif subcmd == "export":
                    if len(parts) < 3:
                        print("‚ö†Ô∏è –£–∫–∞–∂–∏—Ç–µ —Ñ–æ—Ä–º–∞—Ç —ç–∫—Å–ø–æ—Ä—Ç–∞: md –∏–ª–∏ csv")
                    elif parts[2].lower() == "md":
                        export_md()
                    elif parts[2].lower() == "csv":
                        export_csv()
                    else:
                        print("‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ md –∏–ª–∏ csv.")
                else:
                    print("‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞ mem.")
            elif cmd == "add" and len(parts) > 1 and parts[1].lower() == "fact":
                add_fact(parts[2:])
            elif cmd == "del" and len(parts) > 1 and parts[1].lower() == "fact":
                del_fact(parts[2:])
            elif cmd == "test":
                phrase = " ".join(parts[1:])
                tokens = tokenizer.tokenize(phrase)
                p_type = classifier.classify(phrase)
                print(f"[Test] –¢–∏–ø: {p_type.name}")
                print(f"[Test] –¢–æ–∫–µ–Ω—ã: {tokens}")
            else:
                phrase = line
                tokens = tokenizer.tokenize(phrase)
                p_type = classifier.classify(phrase)
                print(f"[Debug] –¢–∏–ø: {p_type.name}")
                print(f"[Debug] –¢–æ–∫–µ–Ω—ã: {tokens}")
                facts = memory.find_facts_by_subject(tokens[0]) if tokens else []
                if facts:
                    print(f"[Debug] –§–∞–∫—Ç—ã –ø–æ '{tokens[0]}':")
                    for f in facts:
                        print(f"  - {f.subject} ‚Äî {f.predicate} ‚Äî {f.obj}")
                else:
                    print("[Debug] –§–∞–∫—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        except KeyboardInterrupt:
            print("\nüõë –í—ã—Ö–æ–¥.")
            break
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    main()