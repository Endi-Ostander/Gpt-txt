

===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\config.py =====

# config.py

MEMORY_PATH = "data/memory.json"
KNOWLEDGE_PATH = "data/knowledge_base.json"
LOG_PATH = "data/logs/ai.log"
LOG_LEVEL = "DEBUG"


===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\README.md =====



===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\requirements.txt =====

pyspellchecker
pytest
beautifulsoup4


===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\run.py =====

from core.learner.trainer import Trainer
from tools.dev_console_ext import print_facts, print_knowledge, export_md, export_csv
import sys

def main():
    trainer = Trainer()
    print("–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ò–ò (–∫–æ–º–∞–Ω–¥—ã –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å /, exit ‚Äî –≤—ã—Ö–æ–¥):")

    while True:
        user_input = input("> ").strip()
        if not user_input:
            continue

        if user_input.startswith("/"):
            parts = user_input[1:].split()
            cmd = parts[0].lower()

            if cmd == "exit":
                break
            elif cmd == "mem":
                if len(parts) == 1:
                    print("‚ùó –£—Ç–æ—á–Ω–∏—Ç–µ: facts, knowledge –∏–ª–∏ export")
                    continue
                subcmd = parts[1].lower()

                if subcmd == "facts":
                    subj = parts[2] if len(parts) > 2 else None
                    print_facts(subj)
                elif subcmd == "knowledge":
                    tag = parts[2] if len(parts) > 2 else None
                    print_knowledge(tag)
                elif subcmd == "export":
                    if len(parts) < 3:
                        print("‚ùó –£–∫–∞–∂–∏—Ç–µ —Ñ–æ—Ä–º–∞—Ç: md –∏–ª–∏ csv")
                    elif parts[2] == "md":
                        export_md()
                    elif parts[2] == "csv":
                        export_csv()
                    else:
                        print("‚ùó –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ md –∏–ª–∏ csv.")
                else:
                    print(f"‚ùó –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞: {subcmd}")
            else:
                print(f"‚ùó –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞: /{cmd}")
        else:
            trainer.process_text(user_input)

if __name__ == "__main__":
    main()


===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\.pytest_cache\README.md =====

# pytest cache directory #

This directory contains data from the pytest's cache plugin,
which provides the `--lf` and `--ff` options, as well as the `cache` fixture.

**Do not** commit this to version control.

See [the docs](https://docs.pytest.org/en/stable/how-to/cache.html) for more information.


===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\core\common\log.py =====

import logging
import os

LOG_DIR = "data/logs"
os.makedirs(LOG_DIR, exist_ok=True)

logging.basicConfig(
    filename=os.path.join(LOG_DIR, "ai.log"),
    filemode='a',
    format='%(asctime)s | %(levelname)s | %(message)s',
    level=logging.DEBUG
)

def log_info(message: str):
    logging.info(message)

def log_error(message: str):
    logging.error(message)

def log_debug(message: str):
    logging.debug(message)


===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\core\common\types.py =====

from dataclasses import dataclass
from enum import Enum
from typing import List, Dict, Any, Optional


class PhraseType(Enum):
    """–¢–∏–ø—ã —Ñ—Ä–∞–∑, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –ò–ò."""
    STATEMENT = "statement"    # –£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
    QUESTION = "question"      # –í–æ–ø—Ä–æ—Å
    COMMAND = "command"        # –ö–æ–º–∞–Ω–¥–∞
    UNKNOWN = "unknown"        # –ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ


class KnowledgeType(Enum):
    """–¢–∏–ø—ã –∑–Ω–∞–Ω–∏–π, —Å–æ—Ö—Ä–∞–Ω—è–µ–º—ã—Ö –≤ –ø–∞–º—è—Ç—å."""
    FACT = "fact"
    CONCEPT = "concept"
    DEFINITION = "definition"
    RULE = "rule"
    EVENT = "event"


@dataclass
class Phrase:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤—Ö–æ–¥–Ω–æ–π —Ñ—Ä–∞–∑—ã."""
    text: str
    tokens: List[str]
    phrase_type: PhraseType
    intent: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None


@dataclass
class Fact:
    """–§–∞–∫—Ç, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –ø–∞–º—è—Ç—å."""
    id: str
    subject: str
    predicate: str
    obj: str
    source: Optional[str]
    timestamp: str


@dataclass
class KnowledgeEntry:
    """–•—Ä–∞–Ω–∏–ª–∏—â–µ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."""
    id: str
    title: str
    content: str
    type: KnowledgeType
    tags: List[str]
    created: str


===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\core\common\utils.py =====

# core/common/utils.py

import json
import os
import re
import uuid
from datetime import datetime, timezone

def resource_path(relative_path: str) -> str:
    """
    –ê–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ project_root (–∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞).
    –†–∞–±–æ—Ç–∞–µ—Ç, –¥–∞–∂–µ –µ—Å–ª–∏ —Å–∫—Ä–∏–ø—Ç –∑–∞–ø—É—â–µ–Ω –∏–∑ –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, tools/).
    """
    base_path = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))  # –∏–∑ core/common ‚Üí project_root
    return os.path.join(base_path, relative_path)

def clean_text(text: str) -> str:
    """–£–¥–∞–ª—è–µ—Ç –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã, —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã –∏ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É."""
    text = re.sub(r'[^\w\s]', '', text)  # –£–¥–∞–ª–∏—Ç—å –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
    return re.sub(r'\s+', ' ', text).strip().lower()

def normalize_whitespace(text: str) -> str:
    """–ü—Ä–∏–≤–æ–¥–∏—Ç –≤—Å–µ –ø—Ä–æ–±–µ–ª—ã –∫ –æ–¥–Ω–æ–º—É."""
    return re.sub(r'\s+', ' ', text).strip()

def generate_id() -> str:
    """–°–æ–∑–¥–∞—ë—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä."""
    return str(uuid.uuid4())

def timestamp() -> str:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â—É—é –¥–∞—Ç—É –∏ –≤—Ä–µ–º—è –≤ ISO —Ñ–æ—Ä–º–∞—Ç–µ."""
    return datetime.now(timezone.utc).isoformat()

def load_json(filepath: str) -> dict:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç JSON-—Ñ–∞–π–ª –≤ —Å–ª–æ–≤–∞—Ä—å."""
    if not os.path.exists(filepath):
        return {}
    with open(filepath, 'r', encoding='utf-8') as f:
        return json.load(f)

def save_json(filepath: str, data: dict) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–ª–æ–≤–∞—Ä—å –≤ JSON-—Ñ–∞–π–ª."""
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=4, ensure_ascii=False)

def pretty_print(obj):
    """–£–¥–æ–±–Ω—ã–π –≤—ã–≤–æ–¥ —Å–ª–æ–≤–∞—Ä—è."""
    print(json.dumps(obj, indent=4, ensure_ascii=False))



===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\core\inputs\html_scraper.py =====

import requests
from bs4 import BeautifulSoup

class HtmlScraper:
    """
    –ú–æ–¥—É–ª—å –¥–ª—è —Å–±–æ—Ä–∞ —Ç–µ–∫—Å—Ç–æ–≤ —Å HTML-—Å—Ç—Ä–∞–Ω–∏—Ü.
    –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ç–µ–≥–∞ <body>.
    """
    def fetch_text(self, url: str) -> str:
        try:
            response = requests.get(url)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, "html.parser")
                body = soup.body
                return body.get_text(separator=' ', strip=True) if body else ""
            else:
                print(f"[HtmlScraper] –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {response.status_code}")
                return ""
        except Exception as e:
            print(f"[HtmlScraper] –ò—Å–∫–ª—é—á–µ–Ω–∏–µ: {e}")
            return ""


===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\core\inputs\text_input.py =====

class TextInput:
    """
    –ú–æ–¥—É–ª—å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –≤–≤–æ–¥–∞ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
    –ü–æ–∫–∞ –ø—Ä–æ—Å—Ç–æ –∑–∞–≥–ª—É—à–∫–∞ –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ –≤–≤–æ–¥–∞.
    """
    def get_input(self) -> str:
        return input("–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –ò–ò: ")


===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\core\learner\curiosity.py =====

class Curiosity:
    def __init__(self):
        self.unknown_phrases = []

    def add_unknown(self, phrase: str):
        self.unknown_phrases.append(phrase)
        print(f"[Curiosity] –ù–æ–≤–∞—è –Ω–µ–ø–æ–Ω—è—Ç–Ω–∞—è —Ñ—Ä–∞–∑–∞: '{phrase}'")

    def get_questions(self):
        return [f"–ß—Ç–æ –∑–Ω–∞—á–∏—Ç: '{p}'" for p in self.unknown_phrases]


    def clear(self):
        self.unknown_phrases.clear()


===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\core\learner\reinforce.py =====

class Reinforce:
    """
    –ú–æ–¥—É–ª—å –¥–ª—è –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –ò–ò.
    –ü–æ–∫–∞ –∑–∞–≥–ª—É—à–∫–∞ ‚Äî —Å—é–¥–∞ –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É —É—Å–∏–ª–µ–Ω–∏—è –∏–∑—É—á–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π,
    –Ω–∞–ø—Ä–∏–º–µ—Ä, —É—Å–∏–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –ø—Ä–∞–≤–∏–ª –∏–ª–∏ —Ñ–∞–∫—Ç–æ–≤ –ø–æ —á–∞—Å—Ç–æ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.
    """
    def __init__(self):
        self.rewards = {}

    def reinforce_fact(self, fact_id: str):
        self.rewards[fact_id] = self.rewards.get(fact_id, 0) + 1
        print(f"[Reinforce] –£—Å–∏–ª–µ–Ω–∏–µ —Ñ–∞–∫—Ç–∞ {fact_id}: {self.rewards[fact_id]}")

    def get_reward(self, fact_id: str) -> int:
        return self.rewards.get(fact_id, 0)


===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\core\learner\rules.py =====

from typing import List, Optional, Dict, Any
from core.common.types import PhraseType

class Rule:
    """
    –ü—Ä–∞–≤–∏–ª–æ –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ —Ñ—Ä–∞–∑—ã –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤ –∑–Ω–∞–Ω–∏–µ –∏–ª–∏ –¥–µ–π—Å—Ç–≤–∏–µ.
    """
    def __init__(self, name: str, condition: callable, action: callable):
        self.name = name
        self.condition = condition  # –§—É–Ω–∫—Ü–∏—è: –ø—Ä–∏–Ω–∏–º–∞–µ—Ç (phrase_type, tokens, raw_text), –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç bool
        self.action = action        # –§—É–Ω–∫—Ü–∏—è: –ø—Ä–∏–Ω–∏–º–∞–µ—Ç (phrase_type, tokens, raw_text), –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç


class RulesEngine:
    def __init__(self):
        self.rules: List[Rule] = []

    def add_rule(self, rule: Rule):
        self.rules.append(rule)

    def apply_rules(self, phrase_type: PhraseType, tokens: List[str], raw_text: str) -> Optional[Dict[str, Any]]:
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç –ø—Ä–∞–≤–∏–ª–∞ –∫ —Ñ—Ä–∞–∑–µ. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–µ—Ä–≤–æ–≥–æ –ø—Ä–∏–º–µ–Ω–∏–º–æ–≥–æ –ø—Ä–∞–≤–∏–ª–∞.
        """
        for rule in self.rules:
            if rule.condition(phrase_type, tokens, raw_text):
                return rule.action(phrase_type, tokens, raw_text)
        return None


# –£—Å–ª–æ–≤–∏–µ –∏ –¥–µ–π—Å—Ç–≤–∏–µ –¥–ª—è —Ñ–∞–∫—Ç–∞ —Å 3 –∏ –±–æ–ª–µ–µ —Ç–æ–∫–µ–Ω–∞–º–∏
def condition_fact_long(phrase_type: PhraseType, tokens: List[str], raw_text: str) -> bool:
    return phrase_type == PhraseType.STATEMENT and len(tokens) >= 3

def action_fact_long(phrase_type: PhraseType, tokens: List[str], raw_text: str) -> Dict[str, Any]:
    subject = tokens[0]
    predicate = tokens[1]
    obj = " ".join(tokens[2:])
    return {"type": "fact", "subject": subject, "predicate": predicate, "object": obj, "source": raw_text}

# –£—Å–ª–æ–≤–∏–µ –∏ –¥–µ–π—Å—Ç–≤–∏–µ –¥–ª—è —Ñ–∞–∫—Ç–∞ —Å —Ä–æ–≤–Ω–æ 2 —Ç–æ–∫–µ–Ω–∞–º–∏
def condition_fact_short(phrase_type: PhraseType, tokens: List[str], raw_text: str) -> bool:
    return phrase_type == PhraseType.STATEMENT and len(tokens) == 2

def action_fact_short(phrase_type: PhraseType, tokens: List[str], raw_text: str) -> Dict[str, Any]:
    subject = tokens[0]
    predicate = tokens[1]
    obj = ""  # –û–±—ä–µ–∫—Ç–∞ –Ω–µ—Ç
    return {"type": "fact", "subject": subject, "predicate": predicate, "object": obj, "source": raw_text}


# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–≤–∏–∂–∫–∞ –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª –≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
rules_engine = RulesEngine()
rules_engine.add_rule(Rule("fact_rule_long", condition_fact_long, action_fact_long))
rules_engine.add_rule(Rule("fact_rule_short", condition_fact_short, action_fact_short))
# ...–≤–Ω–∏–∑—É rules.py

def condition_question(phrase_type, tokens, raw_text):
    return phrase_type == PhraseType.QUESTION

def action_question(phrase_type, tokens, raw_text):
    return {
        "type": "question",
        "text": raw_text,
        "tokens": tokens
    }

# –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∞–≤–∏–ª–æ (–ø–æ—Å–ª–µ —Ñ–∞–∫—Ç–æ–≤)
rules_engine.add_rule(Rule("question_rule", condition_question, action_question))


===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\core\learner\trainer.py =====

# core/learner/trainer.py
from core.processor.tokenizer import Tokenizer
from core.processor.classifier import Classifier
from core.memory.memory import Memory
from core.common.types import PhraseType
from core.learner.rules import rules_engine
from core.learner.curiosity import Curiosity
from core.processor.spellchecker import SpellCorrector
from core.common.log import log_info, log_error
from core.common.utils import resource_path



class Trainer:
    def __init__(self):
        self.tokenizer = Tokenizer()
        self.classifier = Classifier()
        self.memory = Memory(
            memory_path=resource_path("data/memory.json"),
            knowledge_path=resource_path("data/knowledge_base.json")
        )
        self.curiosity = Curiosity()
        self.spellchecker = SpellCorrector()

    def process_text(self, text: str):
        """–ì–ª–∞–≤–Ω–∞—è —Ç–æ—á–∫–∞ –æ–±—É—á–µ–Ω–∏—è: –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        phrase_type = self.classifier.classify(text)
        tokens = self.tokenizer.tokenize(text)

        print(f"[Trainer] –¢–∏–ø —Ñ—Ä–∞–∑—ã: {phrase_type.name}")
        print(f"[Trainer] –¢–æ–∫–µ–Ω—ã: {tokens}")

        corrected = self.spellchecker.correct_tokens(tokens)

        if corrected != tokens:
            print(f"[Spell] –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: {tokens} ‚Üí {corrected}")
            tokens = corrected

        result = rules_engine.apply_rules(phrase_type, tokens, text)
        if result:
            if result.get("type") == "fact":
                added = self.memory.add_fact(
                    result["subject"],
                    result["predicate"],
                    result["object"],
                    source=result.get("source")
                )
                if added:
                    print(f"[Trainer] –ó–∞–ø–æ–º–Ω–µ–Ω–æ: {result['subject']} ‚Äî {result['predicate']} ‚Äî {result['object']}")
                    log_info(f"–§–∞–∫—Ç –¥–æ–±–∞–≤–ª–µ–Ω: {result['subject']} ‚Äî {result['predicate']} ‚Äî {result['object']}")
            elif result.get("type") == "question":
                print(f"[Trainer] –í–æ–ø—Ä–æ—Å: {result['text']}")
                log_info(f"–í–æ–ø—Ä–æ—Å: {result['text']}")
            elif result.get("type") == "concept":
                self.memory.add_knowledge(
                    title=result["title"],
                    content=result["content"],
                    k_type=result["k_type"],
                    tags=result.get("tags", [])
                )
                print(f"[Trainer] –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∫–∞–∫ –∑–Ω–∞–Ω–∏–µ: {result['title']}")
        else:
            self.curiosity.add_unknown(text)
            print("[Trainer] –ù–µ–ø–æ–Ω—è—Ç–Ω–∞—è —Ñ—Ä–∞–∑–∞ ‚Äî –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤ —Å–ø–∏—Å–æ–∫ –≤–æ–ø—Ä–æ—Å–æ–≤.")
            log_error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å: {text}")


===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\core\memory\memory.py =====

import os
from typing import List, Optional
from core.common.utils import load_json, save_json, generate_id, timestamp
from core.common.types import Fact, KnowledgeEntry, KnowledgeType
from core.common.utils import resource_path

MEMORY_FILE = resource_path("data/memory.json")
KNOWLEDGE_FILE = resource_path("data/knowledge_base.json")


class Memory:
    def __init__(self, memory_path: Optional[str] = None, knowledge_path: Optional[str] = None):
        self.memory_path = memory_path or MEMORY_FILE
        self.knowledge_path = knowledge_path or KNOWLEDGE_FILE

        self.facts = self._load_facts()
        self.knowledge = self._load_knowledge()

    def _load_facts(self) -> List[Fact]:
        if not os.path.exists(self.memory_path):
            # –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
            return []
        
        data = load_json(self.memory_path)
        return [Fact(**f) for f in data.get("facts", [])]

    def _load_knowledge(self) -> List[KnowledgeEntry]:
        if not os.path.exists(self.knowledge_path):
            # –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
            return []
        
        data = load_json(self.knowledge_path)
        return [KnowledgeEntry(**k) for k in data.get("entries", [])]

    def _save_facts(self):
        data = {"facts": [f.__dict__ for f in self.facts]}
        save_json(self.memory_path, data)

    def _save_knowledge(self):
        data = {
            "entries": [
                {
                    **k.__dict__,
                    "type": k.type.value if hasattr(k.type, "value") else k.type
                }
                for k in self.knowledge
            ]
        }
        save_json(self.knowledge_path, data)

    def add_fact(self, subject: str, predicate: str, obj: str, source: Optional[str] = None) -> bool:
        for f in self.facts:
            if f.subject == subject and f.predicate == predicate and f.obj == obj:
                print(f"[Memory] –§–∞–∫—Ç —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {subject} ‚Äî {predicate} ‚Äî {obj}")
                return False

        fact = Fact(
            id=generate_id(),
            subject=subject,
            predicate=predicate,
            obj=obj,
            source=source,
            timestamp=timestamp()
        )
        self.facts.append(fact)
        self._save_facts()
        print(f"[Memory] –ù–æ–≤—ã–π —Ñ–∞–∫—Ç –¥–æ–±–∞–≤–ª–µ–Ω: {subject} ‚Äî {predicate} ‚Äî {obj}")
        return True

    def add_knowledge(self, title: str, content: str, k_type: KnowledgeType, tags: List[str]):
        entry = KnowledgeEntry(
            id=generate_id(),
            title=title,
            content=content,
            type=k_type,
            tags=tags,
            created=timestamp()
        )
        self.knowledge.append(entry)
        self._save_knowledge()

    def find_facts_by_subject(self, subject: str) -> List[Fact]:
        return [f for f in self.facts if f.subject == subject]

    def find_knowledge_by_tag(self, tag: str) -> List[KnowledgeEntry]:
        return [k for k in self.knowledge if tag in k.tags]


===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\core\memory\recall.py =====

from core.memory.memory import Memory

class Recall:
    """
    –ú–æ–¥—É–ª—å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –ø–∞–º—è—Ç–∏.
    –î–µ–ª–µ–≥–∏—Ä—É–µ—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç—É Memory.
    """
    def __init__(self):
        self.memory = Memory()

    def recall_facts(self, subject: str):
        return self.memory.find_facts_by_subject(subject)

    def recall_knowledge(self, tag: str):
        return self.memory.find_knowledge_by_tag(tag)


===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\core\memory\update.py =====

from core.memory.memory import Memory
from core.common.types import Fact

class UpdateMemory:
    """
    –ú–æ–¥—É–ª—å –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∞–∫—Ç–æ–≤ –≤ –ø–∞–º—è—Ç–∏.
    –ü–æ–∫–∞ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –ø—Ä–æ—Å—Ç–∞—è –∑–∞–º–µ–Ω–∞ —Ñ–∞–∫—Ç–∞ –ø–æ ID.
    """
    def __init__(self):
        self.memory = Memory()

    def update_fact(self, fact_id: str, new_subject: str, new_predicate: str, new_obj: str):
        updated = False
        for i, fact in enumerate(self.memory.facts):
            if fact.id == fact_id:
                self.memory.facts[i] = Fact(fact_id, new_subject, new_predicate, new_obj, fact.source, fact.timestamp)
                updated = True
                break
        if updated:
            self.memory._save_facts()
            print(f"[UpdateMemory] –§–∞–∫—Ç {fact_id} –æ–±–Ω–æ–≤–ª–µ–Ω.")
        else:
            print(f"[UpdateMemory] –§–∞–∫—Ç {fact_id} –Ω–µ –Ω–∞–π–¥–µ–Ω.")


===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\core\processor\classifier.py =====

# core/processor/classifier.py

from core.common.types import PhraseType


class Classifier:
    def __init__(self):
        self.question_starters = {"—á—Ç–æ", "–∫—Ç–æ", "–≥–¥–µ", "–ø–æ—á–µ–º—É", "–∑–∞—á–µ–º", "–∫–∞–∫", "–∫–æ–≥–¥–∞", "—Å–∫–æ–ª—å–∫–æ", "–º–æ–∂–Ω–æ", "–ª–∏"}
        self.command_verbs = {"—Ä–∞—Å—Å–∫–∞–∂–∏", "–ø–æ–∫–∞–∂–∏", "—Å–∫–∞–∂–∏", "–æ–±—ä—è—Å–Ω–∏", "–∑–∞–ø–æ–º–Ω–∏", "–æ—Ç–≤–µ—Ç—å", "–Ω–∞–π–¥–∏"}

    def classify(self, text: str) -> PhraseType:
        lowered = text.strip().lower()
        if not text:
            return PhraseType.UNKNOWN
        if text.startswith("/"):
            return PhraseType.COMMAND
        if all(c == '?' for c in lowered):
            return PhraseType.UNKNOWN

        if lowered.endswith("?"):
            return PhraseType.QUESTION

        if any(lowered.startswith(q) for q in self.question_starters):
            return PhraseType.QUESTION

        if any(lowered.startswith(cmd) for cmd in self.command_verbs):
            return PhraseType.COMMAND

        if lowered.endswith(".") or " " in lowered:
            return PhraseType.STATEMENT

        return PhraseType.UNKNOWN


===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\core\processor\extractor.py =====

from typing import Dict, Any, Optional

class Extractor:
    """
    –ú–æ–¥—É–ª—å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ —Ç–æ–∫–µ–Ω–æ–≤ –∏ —Ç–µ–∫—Å—Ç–∞.
    –ù–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ ‚Äî –ø—Ä–æ—Å—Ç–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å.
    """
    def extract_fact(self, tokens: list, raw_text: str) -> Optional[Dict[str, Any]]:
        if len(tokens) < 3:
            return None
        return {
            "subject": tokens[0],
            "predicate": tokens[1],
            "object": " ".join(tokens[2:]),
            "source": raw_text
        }


===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\core\processor\parser.py =====

# core/processor/parser.py

from core.common.types import Phrase, PhraseType
from core.processor.tokenizer import Tokenizer

class Parser:
    def __init__(self):
        self.tokenizer = Tokenizer()

    def parse(self, text: str) -> Phrase:
        tokens = self.tokenizer.tokenize(text)
        phrase_type = PhraseType.STATEMENT  # –≤—Ä–µ–º–µ–Ω–Ω–æ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        return Phrase(text=text, tokens=tokens, phrase_type=phrase_type)


===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\core\processor\spellchecker.py =====

from spellchecker import SpellChecker

class SpellCorrector:
    def __init__(self, language="ru"):
        self.spell = SpellChecker(language=language)

    def correct_tokens(self, tokens: list) -> list:
        corrected = []
        for word in tokens:
            correction = self.spell.correction(word)
            corrected.append(correction if correction else word)
        return corrected


===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\core\processor\tokenizer.py =====

import re
from typing import List
from core.common.utils import clean_text


class Tokenizer:
    def __init__(self):
        # –ú–æ–∂–Ω–æ –ø–æ–∑–∂–µ –¥–æ–±–∞–≤–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤
        self.delimiters = r"[ \t\n\r\f\v.,!?;:\"()\-‚Äî]+"  # –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ —Å–ª–æ–≤

    def tokenize(self, text: str) -> List[str]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —Å—Ç—Ä–æ–∫—É –Ω–∞ —Ç–æ–∫–µ–Ω—ã."""
        cleaned = clean_text(text)
        tokens = re.split(self.delimiters, cleaned)
        return [t for t in tokens if t]  # –£–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏

    def count_tokens(self, text: str) -> int:
        """–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤."""
        return len(self.tokenize(text))

    def preview_tokens(self, text: str) -> None:
        """–í—ã–≤–æ–¥–∏—Ç —Ç–æ–∫–µ–Ω—ã –≤ –∫–æ–Ω—Å–æ–ª—å –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏."""
        tokens = self.tokenize(text)
        print(f"[Tokenizer] ‚Üí {tokens}")


===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\data\knowledge_base.json =====

{
    "entries": [
        {
            "id": "6a2b92d5-003d-4677-9ac8-6dcfa911ca99",
            "title": "–¢–µ—Å—Ç",
            "content": "–û–ø–∏—Å–∞–Ω–∏–µ",
            "type": "fact",
            "tags": [
                "—Ç–µ—Å—Ç"
            ],
            "created": "2025-06-25T19:47:05.917364"
        },
        {
            "id": "e41b0ce8-843b-4715-a75e-1fde8280b642",
            "title": "–¢–µ—Å—Ç",
            "content": "–û–ø–∏—Å–∞–Ω–∏–µ",
            "type": "fact",
            "tags": [
                "—Ç–µ—Å—Ç"
            ],
            "created": "2025-06-25T19:50:27.315605+00:00"
        },
        {
            "id": "7d57d090-0c66-4c30-8e6a-c9d19044255a",
            "title": "–¢–µ—Å—Ç",
            "content": "–û–ø–∏—Å–∞–Ω–∏–µ",
            "type": "fact",
            "tags": [
                "—Ç–µ—Å—Ç"
            ],
            "created": "2025-06-25T19:53:26.690630+00:00"
        }
    ]
}

===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\data\memory.json =====

{
    "facts": [
        {
            "id": "22222222-2222-2222-2222-222222222222",
            "subject": "–≤–æ–¥–∞",
            "predicate": "—Å–æ—Å—Ç–æ–∏—Ç –∏–∑",
            "obj": "–≤–æ–¥–æ—Ä–æ–¥–∞ –∏ –∫–∏—Å–ª–æ—Ä–æ–¥–∞",
            "source": "–Ω–∞—á–∞–ª—å–Ω—ã–µ –∑–Ω–∞–Ω–∏—è",
            "timestamp": "2025-06-25T12:00:00"
        },
        {
            "id": "5d113417-ab1b-42d5-93a5-c1e703651742",
            "subject": "–¥–µ—Ä–µ–≤—å—è",
            "predicate": "–º–æ–≥—É—Ç",
            "obj": "–≥–æ—Ä–µ—Ç—å",
            "source": "–î–µ—Ä–µ–≤—å—è –º–æ–≥—É—Ç –≥–æ—Ä–µ—Ç—å",
            "timestamp": "2025-06-25T18:18:49.854477"
        },
        {
            "id": "67bced0c-d0ce-47cb-86ef-089d35bceeea",
            "subject": "–¥–µ—Ä–µ–≤–æ",
            "predicate": "–≥–æ—Ä–∏—Ç",
            "obj": "",
            "source": "–î–µ—Ä–µ–≤–æ –≥–æ—Ä–∏—Ç",
            "timestamp": "2025-06-25T18:25:34.438213"
        },
        {
            "id": "4e447987-563c-4607-99b2-0db4dec3e842",
            "subject": "–¢–µ—Å—Ç",
            "predicate": "–µ—Å—Ç—å",
            "obj": "–¥–∞–Ω–Ω—ã–µ",
            "source": "—Ç–µ—Å—Ç",
            "timestamp": "2025-06-25T19:26:35.762239"
        },
        {
            "id": "a8c0063d-1108-4741-af2a-000df871027f",
            "subject": "—Å–æ–ª–Ω—Ü–µ",
            "predicate": "—ç—Ç–æ",
            "obj": "–∑–≤–µ–∑–¥–∞",
            "source": "–°–æ–ª–Ω—Ü–µ —ç—Ç–æ –∑–≤–µ–∑–¥–∞",
            "timestamp": "2025-06-25T19:58:12.328126+00:00"
        },
        {
            "id": "61331c5d-071a-4b9d-829c-cadbd8d5ca76",
            "subject": "\"–°–æ–ª–Ω—Ü–µ\"",
            "predicate": "\"–°–æ–ª–Ω—Ü–µ",
            "obj": "- —ç—Ç–æ –∑–≤–µ–∑–¥–∞, –∫–æ—Ç–æ—Ä–∞—è —è–≤–ª—è–µ—Ç—Å—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º —Å–≤–µ—Ç–∞ –∏ —Ç–µ–ø–ª–∞ –¥–ª—è –ó–µ–º–ª–∏.\" [\"–∞—Å—Ç—Ä–æ–Ω–æ–º–∏—è\", \"—Å–æ–ª–Ω—Ü–µ\",",
            "source": "\"–∑–≤–µ–∑–¥—ã\"]",
            "timestamp": "2025-06-25T20:23:55.058045+00:00"
        },
        {
            "id": "e8a2b6ad-3560-47ce-8d66-7dc8287d89ec",
            "subject": "—Ç—ã",
            "predicate": "–º–æ–π",
            "obj": "–ø–æ–º–æ—â—å–Ω—ã–∫",
            "source": "–¢—ã –º–æ–π –ø–æ–º–æ—â—å–Ω—ã–∫",
            "timestamp": "2025-06-26T12:57:27.870724+00:00"
        },
        {
            "id": "8aa2c1c2-2c51-4e2d-894c-4a5e91ae4888",
            "subject": "mem",
            "predicate": "facts",
            "obj": "",
            "source": "mem facts",
            "timestamp": "2025-06-26T13:03:21.926253+00:00"
        },
        {
            "id": "ae477ccc-7500-4cb2-b11b-0974e0e80b9d",
            "subject": "–º—ã",
            "predicate": "–ª—É—á—à—ã–µ",
            "obj": "–¥—Ä—É–∑—å—è",
            "source": "–ú—ã –ª—É—á—à—ã–µ –¥—Ä—É–∑—å—è",
            "timestamp": "2025-06-26T17:15:18.049711+00:00"
        },
        {
            "id": "c18fec1d-5717-48c5-9eaa-e69a09d69d78",
            "subject": "–º—ã",
            "predicate": "–ª—É—á—à–∏–µ",
            "obj": "–¥—Ä—É–∑—å—è",
            "source": "–ú—ã –ª—É—á—à–∏–µ –¥—Ä—É–∑—å—è",
            "timestamp": "2025-06-26T17:15:34.986747+00:00"
        },
        {
            "id": "55608bf1-9e15-49b4-b84c-7171951340de",
            "subject": "–º—ã",
            "predicate": "–ª—É—á—à–µ",
            "obj": "–¥—Ä—É–∑—å—è",
            "source": "–ú—ã –ª—É—á—à—ã–µ –¥—Ä—É–∑—å—è",
            "timestamp": "2025-06-26T17:33:53.085594+00:00"
        }
    ]
}

===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\tests\test_classifier.py =====

import pytest
from core.processor.classifier import Classifier
from core.common.types import PhraseType

classifier = Classifier()

def test_classify_command():
    c = Classifier()
    assert c.classify("/run") == PhraseType.COMMAND

def test_classify_unknown():
    c = Classifier()
    assert c.classify("üò≥?üêç") == PhraseType.UNKNOWN

@pytest.mark.parametrize("text,expected", [
    ("–ß—Ç–æ —Ç—ã –∑–Ω–∞–µ—à—å?", PhraseType.QUESTION),
    ("–°–∫–æ–ª—å–∫–æ –≤—Ä–µ–º–µ–Ω–∏?", PhraseType.QUESTION),
    ("–†–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ —Å–æ–ª–Ω—Ü–µ", PhraseType.COMMAND),
    ("–°–æ–ª–Ω—Ü–µ ‚Äî —ç—Ç–æ –∑–≤–µ–∑–¥–∞.", PhraseType.STATEMENT),
    ("???", PhraseType.UNKNOWN),
])
def test_classify(text, expected):
    assert classifier.classify(text) == expected


===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\tests\test_curiosity.py =====

from core.learner.curiosity import Curiosity

def test_add_and_get_questions():
    c = Curiosity()
    c.add_unknown("–ö–≤–∞–Ω—Ç–æ–≤–∞—è –∑–∞–ø—É—Ç–∞–Ω–Ω–æ—Å—Ç—å?")
    questions = c.get_questions()
    assert questions == ["–ß—Ç–æ –∑–Ω–∞—á–∏—Ç: '–ö–≤–∞–Ω—Ç–æ–≤–∞—è –∑–∞–ø—É—Ç–∞–Ω–Ω–æ—Å—Ç—å?'"]

def test_clear():
    c = Curiosity()
    c.add_unknown("abc")
    c.clear()
    assert c.get_questions() == []


===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\tests\test_html_scraper.py =====

from core.inputs.html_scraper import HtmlScraper

def test_fetch_empty_html(monkeypatch):
    class FakeResponse:
        text = ""
    def fake_get(*args, **kwargs):
        return FakeResponse()
    monkeypatch.setattr("requests.get", fake_get)
    scraper = HtmlScraper()
    result = scraper.fetch_text("http://test")
    assert result == ""

def test_fetch_text_invalid_url():
    scraper = HtmlScraper()
    result = scraper.fetch_text("http://invalid.url")
    assert isinstance(result, str)


===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\tests\test_memory.py =====

import tempfile
import shutil
import os
from core.memory.memory import Memory
from core.common.types import KnowledgeType
from core.common.utils import generate_id, timestamp

def test_find_facts_by_subject(tmp_path):
    memory_file = tmp_path / "memory.json"
    knowledge_file = tmp_path / "knowledge.json"
    memory = Memory(memory_path=str(memory_file), knowledge_path=str(knowledge_file))

    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π —Ñ–∞–∫—Ç
    memory.add_fact("–≤–æ–¥–∞", "—è–≤–ª—è–µ—Ç—Å—è", "–∂–∏–¥–∫–æ—Å—Ç—å—é")
    
    results = memory.find_facts_by_subject("–≤–æ–¥–∞")
    assert len(results) == 1
    assert results[0].subject == "–≤–æ–¥–∞"


def test_find_facts_by_subject(tmp_path):
    mem = Memory(
        memory_path=str(tmp_path / "memory.json"),
        knowledge_path=str(tmp_path / "knowledge.json")
    )
    mem.add_fact("–≤–æ–¥–∞", "—è–≤–ª—è–µ—Ç—Å—è", "–∂–∏–¥–∫–æ—Å—Ç—å—é")
    results = mem.find_facts_by_subject("–≤–æ–¥–∞")
    assert len(results) == 1

def test_memory_fact_storage():
    tmp_dir = tempfile.mkdtemp()
    memory_file = os.path.join(tmp_dir, "memory.json")
    knowledge_file = os.path.join(tmp_dir, "knowledge_base.json")

    # –°–æ–∑–¥–∞—ë–º –ø—É—Å—Ç—ã–µ —Ñ–∞–π–ª—ã –¥–ª—è –ø–∞–º—è—Ç–∏
    with open(memory_file, "w", encoding="utf-8") as f:
        f.write('{"facts": []}')
    with open(knowledge_file, "w", encoding="utf-8") as f:
        f.write('{"entries": []}')

    # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç–∏
    memory = Memory(memory_path=memory_file, knowledge_path=knowledge_file)

    # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∞–∫—Ç
    memory.add_fact("–¢–µ—Å—Ç", "–µ—Å—Ç—å", "–¥–∞–Ω–Ω—ã–µ", "—Ç–µ—Å—Ç")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–∞–∫—Ç –±—ã–ª –¥–æ–±–∞–≤–ª–µ–Ω
    facts = memory.find_facts_by_subject("–¢–µ—Å—Ç")
    assert len(facts) == 1
    assert facts[0].predicate == "–µ—Å—Ç—å"

    shutil.rmtree(tmp_dir)

def test_memory_knowledge_storage():
    tmp_dir = tempfile.mkdtemp()
    memory_file = os.path.join(tmp_dir, "memory.json")
    knowledge_file = os.path.join(tmp_dir, "knowledge_base.json")

    # –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    with open(memory_file, "w", encoding="utf-8") as f:
        f.write('{"facts": []}')
    with open(knowledge_file, "w", encoding="utf-8") as f:
        f.write('{"entries": []}')

    # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç–∏
    memory = Memory(memory_path=memory_file, knowledge_path=knowledge_file)

    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞–Ω–∏—è
    memory.add_knowledge("–¢–µ—Å—Ç", "–û–ø–∏—Å–∞–Ω–∏–µ", KnowledgeType.FACT, ["—Ç–µ—Å—Ç"])
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∑–Ω–∞–Ω–∏—è –¥–æ–±–∞–≤–ª–µ–Ω—ã
    found = memory.find_knowledge_by_tag("—Ç–µ—Å—Ç")
    assert len(found) == 1

    shutil.rmtree(tmp_dir)


===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\tests\test_parser.py =====

from core.processor.parser import Parser
from core.common.types import Phrase

def test_parse_structure():
    parser = Parser()
    phrase = parser.parse("–°–æ–ª–Ω—Ü–µ —ç—Ç–æ –∑–≤–µ–∑–¥–∞")
    assert isinstance(phrase, Phrase)
    assert phrase.tokens == ["—Å–æ–ª–Ω—Ü–µ", "—ç—Ç–æ", "–∑–≤–µ–∑–¥–∞"]  # ‚¨Ö –∑–∞–º–µ–Ω–∏–ª–∏ –Ω–∞ –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä


===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\tests\test_reinforce.py =====

from core.learner.reinforce import Reinforce

def test_reinforce_fact():
    r = Reinforce()
    r.reinforce_fact("f123")
    assert r.get_reward("f123") == 1
    r.reinforce_fact("f123")
    assert r.get_reward("f123") == 2

def test_no_reward():
    r = Reinforce()
    assert r.get_reward("unknown") == 0


===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\tests\test_rules.py =====

from core.learner.rules import rules_engine
from core.common.types import PhraseType

def test_long_fact_rule():
    result = rules_engine.apply_rules(PhraseType.STATEMENT, ["—Å–æ–ª–Ω—Ü–µ", "—è–≤–ª—è–µ—Ç—Å—è", "–∑–≤–µ–∑–¥–æ–π"], "–°–æ–ª–Ω—Ü–µ —è–≤–ª—è–µ—Ç—Å—è –∑–≤–µ–∑–¥–æ–π")
    assert result["type"] == "fact"
    assert result["subject"] == "—Å–æ–ª–Ω—Ü–µ"

def test_short_fact_rule():
    result = rules_engine.apply_rules(PhraseType.STATEMENT, ["–≤–æ–¥–∞", "–º–æ–∫—Ä–∞—è"], "–í–æ–¥–∞ –º–æ–∫—Ä–∞—è")
    assert result["type"] == "fact"
    assert result["subject"] == "–≤–æ–¥–∞"

def test_question_rule():
    result = rules_engine.apply_rules(PhraseType.QUESTION, ["—á—Ç–æ", "—Ç–∞–∫–æ–µ", "–≤–æ–¥–∞"], "–ß—Ç–æ —Ç–∞–∫–æ–µ –≤–æ–¥–∞?")
    assert result["type"] == "question"
    assert result["text"] == "–ß—Ç–æ —Ç–∞–∫–æ–µ –≤–æ–¥–∞?"

def test_no_match_rule():
    result = rules_engine.apply_rules(PhraseType.UNKNOWN, ["..."], "???")
    assert result is None


===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\tests\test_spellchecker.py =====

from core.processor.spellchecker import SpellCorrector

def test_correct_known_word():
    sc = SpellCorrector()
    assert sc.correct_tokens(["–ø–æ–º–æ—â–Ω—ã–∫"]) == ["–ø–æ–º–æ—â–Ω–∏–∫"]

def test_does_not_change_correct():
    sc = SpellCorrector()
    assert sc.correct_tokens(["–¥—Ä—É–∑—å—è"]) == ["–¥—Ä—É–∑—å—è"]

def test_mixed_batch():
    sc = SpellCorrector()
    assert sc.correct_tokens(["–ª—É—á—à—ã–µ", "–ø–æ–º–æ—â–Ω—ã–∫", "–º—ã"]) == ["–ª—É—á—à–µ", "–ø–æ–º–æ—â–Ω–∏–∫", "–º—ã"]


===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\tests\test_text_input.py =====

from core.inputs.text_input import TextInput

def test_text_input_stub():
    ti = TextInput()
    assert hasattr(ti, "get_input")


===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\tests\test_tokenizer.py =====

import pytest
from core.processor.tokenizer import Tokenizer

tokenizer = Tokenizer()

def test_clean_text():
    from core.common.utils import clean_text
    assert clean_text("–ü—Ä–∏–≤–µ—Ç, –º–∏—Ä!!!") == "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"

def test_split_symbols():
    t = Tokenizer()
    result = t.tokenize("–¢—ã ‚Äî –º–æ–π –¥—Ä—É–≥.")
    assert "–¥—Ä—É–≥" in result

def test_tokenize_basic():
    text = "–ü—Ä–∏–≤–µ—Ç, –º–∏—Ä!"
    tokens = tokenizer.tokenize(text)
    assert tokens == ["–ø—Ä–∏–≤–µ—Ç", "–º–∏—Ä"]

def test_tokenize_empty():
    assert tokenizer.tokenize("") == []

def test_tokenize_whitespace():
    assert tokenizer.tokenize("   \n\t") == []


===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\tests\test_utils.py =====

from core.common.utils import clean_text, normalize_whitespace, generate_id, timestamp
import os
from core.common.utils import save_json, load_json

def test_save_and_load_json(tmp_path):
    filepath = tmp_path / "test.json"
    data = {"a": 1}
    save_json(str(filepath), data)
    loaded = load_json(str(filepath))
    assert loaded == data

def test_clean_text():
    text = " –ü—Ä–∏–≤–µ—Ç, –º–∏—Ä!!! "
    assert clean_text(text) == "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"

def test_normalize_whitespace():
    text = "—ç—Ç–æ   —Ç–µ—Å—Ç\n\n—Å—Ç—Ä–æ–∫–∏"
    assert normalize_whitespace(text) == "—ç—Ç–æ —Ç–µ—Å—Ç —Å—Ç—Ä–æ–∫–∏"

def test_generate_id_unique():
    id1 = generate_id()
    id2 = generate_id()
    assert id1 != id2 and isinstance(id1, str)

def test_timestamp_format():
    ts = timestamp()
    assert "T" in ts  # ISO —Ñ–æ—Ä–º–∞—Ç


===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\tests\__init__.py =====



===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\tools\debug_console.py =====

import sys
from core.memory.memory import Memory
from core.processor.tokenizer import Tokenizer
from core.processor.classifier import Classifier

memory = Memory()
tokenizer = Tokenizer()
classifier = Classifier()

print("üîß Debug Console ‚Äî type 'exit' to quit.\n")

while True:
    text = input("> ")
    if text.strip().lower() == "exit":
        break

    tokens = tokenizer.tokenize(text)
    phrase_type = classifier.classify(text)

    print(f"[Debug] –¢–∏–ø: {phrase_type.name}")
    print(f"[Debug] –¢–æ–∫–µ–Ω—ã: {tokens}")

    facts = memory.find_facts_by_subject(tokens[0]) if tokens else []
    if facts:
        print(f"[Debug] –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–∫—Ç–æ–≤ –ø–æ '{tokens[0]}':")
        for f in facts:
            print(f"    - {f.subject} ‚Äî {f.predicate} ‚Äî {f.obj}")
    else:
        print("[Debug] –§–∞–∫—Ç—ã –ø–æ —Å—É–±—ä–µ–∫—Ç—É –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")


===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\tools\dev_console.py =====

import sys
from core.memory.memory import Memory
from core.processor.tokenizer import Tokenizer
from core.processor.classifier import Classifier
from core.common.types import KnowledgeType

memory = Memory()
tokenizer = Tokenizer()
classifier = Classifier()

HELP = """
üß™ Dev Console ‚Äî –∫–æ–º–∞–Ω–¥—ã:
  > help               ‚Äî –ø–æ–∫–∞–∑–∞—Ç—å —Å–ø–∏—Å–æ–∫ –∫–æ–º–∞–Ω–¥
  > exit               ‚Äî –≤—ã–π—Ç–∏
  > mem facts          ‚Äî –ø–æ–∫–∞–∑–∞—Ç—å –≤—Å–µ —Ñ–∞–∫—Ç—ã
  > mem knowledge      ‚Äî –ø–æ–∫–∞–∑–∞—Ç—å –≤—Å–µ –∑–Ω–∞–Ω–∏—è
  > mem export         ‚Äî —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—Ç—á–µ—Ç –≤ memory_report.md
  > test <—Ñ—Ä–∞–∑–∞>       ‚Äî —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
  > <—Ñ—Ä–∞–∑–∞>            ‚Äî –æ–±—ã—á–Ω–∞—è —Ñ—Ä–∞–∑–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
"""

def print_facts():
    if not memory.facts:
        print("‚ùå –§–∞–∫—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        return
    for f in memory.facts:
        print(f"- {f.subject} ‚Äî {f.predicate} ‚Äî {f.obj}  | from: {f.source or 'n/a'}")

def print_knowledge():
    if not memory.knowledge:
        print("‚ùå –ó–Ω–∞–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        return
    for k in memory.knowledge:
        print(f"- {k.title} [{k.type}] ‚Äî {', '.join(k.tags)}")
        print(f"  > {k.content}")

def export_to_md():
    with open("memory_report.md", "w", encoding="utf-8") as f:
        f.write("# üß† –ü–∞–º—è—Ç—å –ò–ò\n\n")
        f.write("## üìå –§–∞–∫—Ç—ã\n\n")
        for fact in memory.facts:
            f.write(f"- **{fact.subject}** ‚Äî *{fact.predicate}* ‚Äî {fact.obj}\n")
            f.write(f"  –ò—Å—Ç–æ—á–Ω–∏–∫: `{fact.source}`\n")
            f.write(f"  –í—Ä–µ–º—è: `{fact.timestamp}`\n\n")
        f.write("## üìö –ó–Ω–∞–Ω–∏—è\n\n")
        for entry in memory.knowledge:
            f.write(f"- **{entry.title}** ({entry.type})\n")
            f.write(f"  > {entry.content}\n")
            f.write(f"  –¢–µ–≥–∏: {', '.join(entry.tags)}\n")
            f.write(f"  –í—Ä–µ–º—è: `{entry.created}`\n\n")
    print("‚úÖ –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: memory_report.md")

print(HELP)
while True:
    try:
        text = input("> ").strip()
        if text.lower() == "exit":
            break
        elif text.lower() == "help":
            print(HELP)
        elif text.lower() == "mem facts":
            print_facts()
        elif text.lower() == "mem knowledge":
            print_knowledge()
        elif text.lower() == "mem export":
            export_to_md()
        elif text.lower().startswith("test "):
            phrase = text[5:]
            tokens = tokenizer.tokenize(phrase)
            p_type = classifier.classify(phrase)
            print(f"[Test] –¢–∏–ø: {p_type.name}")
            print(f"[Test] –¢–æ–∫–µ–Ω—ã: {tokens}")
        else:
            tokens = tokenizer.tokenize(text)
            p_type = classifier.classify(text)
            print(f"[Debug] –¢–∏–ø: {p_type.name}")
            print(f"[Debug] –¢–æ–∫–µ–Ω—ã: {tokens}")
            facts = memory.find_facts_by_subject(tokens[0]) if tokens else []
            if facts:
                print(f"[Debug] –§–∞–∫—Ç—ã –ø–æ '{tokens[0]}':")
                for f in facts:
                    print(f"  - {f.subject} ‚Äî {f.predicate} ‚Äî {f.obj}")
            else:
                print("[Debug] –§–∞–∫—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")

    except KeyboardInterrupt:
        print("\nüõë –í—ã—Ö–æ–¥.")
        break
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")


===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\tools\dev_console_ext.py =====

import sys
import os
import csv
import subprocess

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ core-–º–æ–¥—É–ª–µ–π
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from core.memory.memory import Memory
from core.processor.tokenizer import Tokenizer
from core.processor.classifier import Classifier
from core.common.types import KnowledgeType


def resource_path(relative_path):
    """–ê–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å –∫ —Ä–µ—Å—É—Ä—Å—É –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ project_root/."""
    script_dir = os.path.abspath(os.path.dirname(__file__))
    project_root = os.path.abspath(os.path.join(script_dir, ".."))
    return os.path.join(project_root, relative_path)


# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
memory = Memory(
    memory_path=resource_path("data/memory.json"),
    knowledge_path=resource_path("data/knowledge_base.json")
)

tokenizer = Tokenizer()
classifier = Classifier()

HELP = """
üß™ –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è Dev Console (—Å –∫–æ–º–∞–Ω–¥–∞–º–∏ —á–µ—Ä–µ–∑ /):

  üìÑ –û–±—â–∏–µ –∫–æ–º–∞–Ω–¥—ã:
    /help                        ‚Äî –ø–æ–∫–∞–∑–∞—Ç—å —ç—Ç—É —Å–ø—Ä–∞–≤–∫—É
    /exit                        ‚Äî –≤—ã–π—Ç–∏ –∏–∑ –∫–æ–Ω—Å–æ–ª–∏
    /run                         ‚Äî –∑–∞–ø—É—Å—Ç–∏—Ç—å run.py

  üì¶ –†–∞–±–æ—Ç–∞ —Å –ø–∞–º—è—Ç—å—é:
    /mem facts                   ‚Äî –ø–æ–∫–∞–∑–∞—Ç—å –≤—Å–µ —Ñ–∞–∫—Ç—ã
    /mem facts <—Å—É–±—ä–µ–∫—Ç>         ‚Äî –Ω–∞–π—Ç–∏ —Ñ–∞–∫—Ç—ã –ø–æ —Å—É–±—ä–µ–∫—Ç—É
    /mem knowledge               ‚Äî –ø–æ–∫–∞–∑–∞—Ç—å –≤—Å–µ –∑–Ω–∞–Ω–∏—è
    /mem knowledge <—Ç–µ–≥>         ‚Äî –Ω–∞–π—Ç–∏ –∑–Ω–∞–Ω–∏—è –ø–æ —Ç–µ–≥—É
    /mem export md|csv           ‚Äî —ç–∫—Å–ø–æ—Ä—Ç –ø–∞–º—è—Ç–∏

  ‚ûï –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏ —É–¥–∞–ª–µ–Ω–∏–µ:
    /add fact <subj> <pred> <obj> [source] ‚Äî –¥–æ–±–∞–≤–∏—Ç—å —Ñ–∞–∫—Ç
    /del fact <id>                         ‚Äî —É–¥–∞–ª–∏—Ç—å —Ñ–∞–∫—Ç –ø–æ ID

  üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ:
    /test <—Ñ—Ä–∞–∑–∞>              ‚Äî —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è

  üîç –ü—Ä–∏–º–µ—Ä:
    /add fact —Å–æ–ª–Ω—Ü–µ –µ—Å—Ç—å –∑–≤–µ–∑–¥–∞ –í–∏–∫–∏–ø–µ–¥–∏—è
"""

def print_facts(filter_subj=None):
    filtered = memory.facts
    if filter_subj:
        filtered = [f for f in filtered if filter_subj.lower() in f.subject.lower()]
    if not filtered:
        print("‚ùå –§–∞–∫—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        return
    for f in filtered:
        print(f"- ID:{f.id} | {f.subject} ‚Äî {f.predicate} ‚Äî {f.obj} | from: {f.source or 'n/a'}")

def print_knowledge(filter_tag=None):
    filtered = memory.knowledge
    if filter_tag:
        filtered = [k for k in filtered if filter_tag.lower() in map(str.lower, k.tags)]
    if not filtered:
        print("‚ùå –ó–Ω–∞–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        return
    for k in filtered:
        print(f"- ID:{k.id} | {k.title} [{k.type.value}] ‚Äî {', '.join(k.tags)}")
        print(f"  > {k.content}")

def export_md():
    with open("memory_report.md", "w", encoding="utf-8") as f:
        f.write("# üß† –ü–∞–º—è—Ç—å –ò–ò\n\n## üìå –§–∞–∫—Ç—ã\n\n")
        for fact in memory.facts:
            f.write(f"- **{fact.subject}** ‚Äî *{fact.predicate}* ‚Äî {fact.obj}\n")
            f.write(f"  –ò—Å—Ç–æ—á–Ω–∏–∫: `{fact.source}`\n")
            f.write(f"  –í—Ä–µ–º—è: `{fact.timestamp}`\n\n")
        f.write("## üìö –ó–Ω–∞–Ω–∏—è\n\n")
        for entry in memory.knowledge:
            f.write(f"- **{entry.title}** ({entry.type.value})\n")
            f.write(f"  > {entry.content}\n")
            f.write(f"  –¢–µ–≥–∏: {', '.join(entry.tags)}\n")
            f.write(f"  –í—Ä–µ–º—è: `{entry.created}`\n\n")
    print("‚úÖ –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ memory_report.md")

def export_csv():
    with open("memory_report.csv", "w", encoding="utf-8", newline='') as f:
        writer = csv.writer(f)
        writer.writerow(["Type", "ID", "Subject/Title", "Predicate", "Object/Content", "Source/Tags", "Timestamp/Created"])
        for fact in memory.facts:
            writer.writerow(["Fact", fact.id, fact.subject, fact.predicate, fact.obj, fact.source or "", fact.timestamp])
        for entry in memory.knowledge:
            tags = ";".join(entry.tags)
            writer.writerow(["Knowledge", entry.id, entry.title, "", entry.content, tags, entry.created])
    print("‚úÖ –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ memory_report.csv")

def add_fact(args):
    if len(args) < 3:
        print("‚ö†Ô∏è –ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 3 –∞—Ä–≥—É–º–µ–Ω—Ç–∞: subject predicate object [source]")
        return
    subject = args[0]
    predicate = args[1]
    rest = args[2:]
    obj = " ".join(rest[:-1]) if len(rest) > 1 else rest[0]
    source = rest[-1] if len(rest) > 1 else None
    memory.add_fact(subject, predicate, obj, source)
    print(f"‚úÖ –§–∞–∫—Ç –¥–æ–±–∞–≤–ª–µ–Ω: {subject} ‚Äî {predicate} ‚Äî {obj}")

def del_fact(args):
    if len(args) != 1:
        print("‚ö†Ô∏è –ù—É–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å ID —Ñ–∞–∫—Ç–∞ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è.")
        return
    fid = args[0]
    before = len(memory.facts)
    memory.facts = [f for f in memory.facts if f.id != fid]
    memory._save_facts()
    if len(memory.facts) < before:
        print(f"‚úÖ –§–∞–∫—Ç —Å ID {fid} —É–¥–∞–ª—ë–Ω.")
    else:
        print(f"‚ùå –§–∞–∫—Ç —Å ID {fid} –Ω–µ –Ω–∞–π–¥–µ–Ω.")

def main():
    print(HELP)
    while True:
        try:
            line = input("> ").strip()
            if not line:
                continue

            if line.startswith("/"):
                parts = line[1:].split()
                cmd = parts[0].lower()

                if cmd == "exit":
                    break
                elif cmd == "help":
                    print(HELP)
                elif cmd == "run":
                    run_path = resource_path("run.py")
                    if os.path.exists(run_path):
                        subprocess.run([sys.executable, run_path])
                        # –ü–æ—Å–ª–µ –≤—ã—Ö–æ–¥–∞ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–∞–º—è—Ç—å
                        memory.facts = memory._load_facts()
                        memory.knowledge = memory._load_knowledge()
                    else:
                        print(f"‚ö†Ô∏è –§–∞–π–ª run.py –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏: {run_path}")
                elif cmd == "mem":
                    if len(parts) < 2:
                        print("‚ö†Ô∏è –£–∫–∞–∂–∏—Ç–µ –∞—Ä–≥—É–º–µ–Ω—Ç: facts, knowledge –∏–ª–∏ export")
                        continue
                    subcmd = parts[1].lower()
                    if subcmd == "facts":
                        subj = parts[2] if len(parts) > 2 else None
                        print_facts(subj)
                    elif subcmd == "knowledge":
                        tag = parts[2] if len(parts) > 2 else None
                        print_knowledge(tag)
                    elif subcmd == "export":
                        if len(parts) < 3:
                            print("‚ö†Ô∏è –£–∫–∞–∂–∏—Ç–µ —Ñ–æ—Ä–º–∞—Ç: md –∏–ª–∏ csv")
                        elif parts[2].lower() == "md":
                            export_md()
                        elif parts[2].lower() == "csv":
                            export_csv()
                        else:
                            print("‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ md –∏–ª–∏ csv.")
                    else:
                        print("‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞ mem.")
                elif cmd == "add" and len(parts) > 1 and parts[1].lower() == "fact":
                    add_fact(parts[2:])
                elif cmd == "del" and len(parts) > 1 and parts[1].lower() == "fact":
                    del_fact(parts[2:])
                elif cmd == "test":
                    phrase = " ".join(parts[1:])
                    tokens = tokenizer.tokenize(phrase)
                    p_type = classifier.classify(phrase)
                    print(f"[Test] –¢–∏–ø: {p_type.name}")
                    print(f"[Test] –¢–æ–∫–µ–Ω—ã: {tokens}")
                else:
                    print("‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞. –í–≤–µ–¥–∏—Ç–µ /help –¥–ª—è —Å–ø—Ä–∞–≤–∫–∏.")
            else:
                # –¢–µ—Å—Ç–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞–ø—Ä—è–º—É—é (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ run.py)
                tokens = tokenizer.tokenize(line)
                p_type = classifier.classify(line)
                print(f"[Debug] –¢–∏–ø: {p_type.name}")
                print(f"[Debug] –¢–æ–∫–µ–Ω—ã: {tokens}")
                facts = memory.find_facts_by_subject(tokens[0]) if tokens else []
                if facts:
                    print(f"[Debug] –§–∞–∫—Ç—ã –ø–æ '{tokens[0]}':")
                    for f in facts:
                        print(f"  - {f.subject} ‚Äî {f.predicate} ‚Äî {f.obj}")
                else:
                    print("[Debug] –§–∞–∫—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        except KeyboardInterrupt:
            print("\nüõë –ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ.")
            break
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    main()


===== FILE: C:\Users\Brazers\Desktop\1. —Å—Ç–∞–¥–∏—è1\project_root\tools\generate_memory_doc.py =====

import os
from core.memory.memory import Memory

memory = Memory()

with open("memory_report.md", "w", encoding="utf-8") as f:
    f.write("# üß† –ü–∞–º—è—Ç—å –ò–ò\n\n")

    f.write("## üìå –§–∞–∫—Ç—ã\n\n")
    for fact in memory.facts:
        f.write(f"- **{fact.subject}** ‚Äî *{fact.predicate}* ‚Äî {fact.obj}  \n")
        f.write(f"  –ò—Å—Ç–æ—á–Ω–∏–∫: `{fact.source}`  \n")
        f.write(f"  –í—Ä–µ–º—è: `{fact.timestamp}`\n\n")

    f.write("## üìö –ó–Ω–∞–Ω–∏—è\n\n")
    for entry in memory.knowledge:
        f.write(f"- **{entry.title}** ({entry.type})\n")
        f.write(f"  > {entry.content}  \n")
        f.write(f"  –¢–µ–≥–∏: {', '.join(entry.tags)}  \n")
        f.write(f"  –í—Ä–µ–º—è: `{entry.created}`\n\n")
